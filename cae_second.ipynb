{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "### Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 28\n",
    "SPLIT_SIZE = 4\n",
    "\n",
    "### train_x = train_x / NORMALIZE\n",
    "NORMALIZE = 1.0\n",
    "# NORMALIZE = 255   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Dataset class\n",
    "class Dataset(object) :\n",
    "    \n",
    "    def __init__(self, path, batch_size, shuffle = True, normalize = 255.0) :\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle    = shuffle\n",
    "        self.normalize  = normalize\n",
    "        self.xs, self.ys = self._load_data(path)\n",
    "        \n",
    "        self.data_size = self.xs.shape[0]\n",
    "        self.batch_max = int(math.ceil(self.data_size / float(self.batch_size)))\n",
    "        \n",
    "    def _load_data(self, path) :\n",
    "        reader = csv.reader(open(path), lineterminator = \"\\n\")\n",
    "        data   = []\n",
    "        labels = []\n",
    "        for row in reader :\n",
    "            tmp = np.asarray([float(x) for x in row])\n",
    "            \n",
    "            ### Normalize\n",
    "            tmp = tmp / self.normalize\n",
    "            \n",
    "            data.append(tmp[:INPUT_SIZE*INPUT_SIZE])\n",
    "            labels.append(tmp[INPUT_SIZE*INPUT_SIZE:])\n",
    "        \n",
    "        data   = np.asarray(data)\n",
    "        labels = np.asarray(labels)\n",
    "        return (data, labels)\n",
    "\n",
    "    def __call__(self) :\n",
    "        indexes = None\n",
    "        if self.shuffle :\n",
    "            indexes = np.random.permutation(self.data_size)\n",
    "        else :\n",
    "            indexes = np.arange(self.data_size)\n",
    "\n",
    "        for d_idx in range(self.batch_max) :\n",
    "            start =     (d_idx + 0) * self.batch_size\n",
    "            end   = min((d_idx + 1) * self.batch_size, self.data_size)\n",
    "            \n",
    "            batch_data_size = len(indexes[start:end])\n",
    "            xs = np.zeros((batch_data_size, self.xs.shape[1]), dtype = np.float32)\n",
    "            ys = np.zeros((batch_data_size, self.ys.shape[1]), dtype = np.float32)\n",
    "            \n",
    "            for b_idx, r_idx in enumerate(indexes[start:end]) :\n",
    "                xs[b_idx] = self.xs[r_idx]\n",
    "                ys[b_idx] = self.ys[r_idx]\n",
    "\n",
    "            yield (xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load dataset\n",
    "dataset = Dataset(\"training_data_4x4.csv\", 10, shuffle = True, normalize = NORMALIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_dataset = Dataset(\"training_data_4x4.csv\", 10, shuffle = False, normalize = NORMALIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load batch data sample code\n",
    "# for batch_num, (train_x, train_y) in enumerate(dataset(), 1) :\n",
    "#     print(\"Batch {0}\".format(batch_num))\n",
    "#     print(\"Train shape : {0}\".format(train_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### for jupyter, interactive session.\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer \"Reshape_2:0\" shape : (?, 28, 28, 1)\n",
      "Layer \"Conv_4/Relu:0\" shape : (?, 28, 28, 32)\n",
      "Layer \"MaxPool2D_3/MaxPool:0\" shape : (?, 14, 14, 32)\n",
      "Layer \"Conv_5/Relu:0\" shape : (?, 14, 14, 64)\n",
      "Layer \"MaxPool2D_4/MaxPool:0\" shape : (?, 7, 7, 64)\n",
      "Layer \"Conv_6/Relu:0\" shape : (?, 7, 7, 64)\n",
      "Layer \"MaxPool2D_5/MaxPool:0\" shape : (?, 4, 4, 64)\n",
      "Layer \"Flatten_2/Reshape:0\" shape : (?, 1024)\n",
      "Layer \"Reshape_3:0\" shape : (?, 4, 4, 1)\n",
      "Layer \"Conv_7/BiasAdd:0\" shape : (?, 4, 4, 1)\n",
      "Layer \"Flatten_3/Reshape:0\" shape : (?, 16)\n"
     ]
    }
   ],
   "source": [
    "### Network settings\n",
    "NN_INPUT_SIZE  = INPUT_SIZE * INPUT_SIZE\n",
    "NN_OUTPUT_SIZE = 4 * SPLIT_SIZE\n",
    "\n",
    "### Input and Output\n",
    "x  = tf.placeholder(tf.float32, shape=[None, NN_INPUT_SIZE], name = 'data')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, NN_OUTPUT_SIZE], name = 'label')\n",
    "is_training = tf.placeholder(tf.bool, [])\n",
    "\n",
    "def print_shape(layer) :\n",
    "    print(\"Layer \\\"{0}\\\" shape : {1}\".format(layer.name, layer.shape))\n",
    "\n",
    "with slim.arg_scope([slim.conv2d, slim.fully_connected], \n",
    "                    activation_fn=tf.nn.relu, \n",
    "                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                    biases_initializer=tf.constant_initializer(0.1),\n",
    "                    normalizer_fn = slim.batch_norm,\n",
    "                    normalizer_params = {'is_training': is_training},\n",
    "                   ), \\\n",
    "    slim.arg_scope([slim.max_pool2d], padding='SAME'), \\\n",
    "    slim.arg_scope([slim.conv2d_transpose], \n",
    "                   padding='SAME', \n",
    "                   activation_fn = None, \n",
    "                   stride = 2, \n",
    "                   weights_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                   normalizer_fn = slim.batch_norm,\n",
    "                   normalizer_params = {'is_training': is_training}\n",
    "                  ) :\n",
    "        \n",
    "        x_image = slim.array_ops.reshape(x, [-1, 28, 28, 1])\n",
    "        print_shape(x_image)\n",
    "        \n",
    "        ### Convolution\n",
    "        h_conv1 = slim.conv2d(x_image, 32, [3, 3])\n",
    "        h_pool1 = slim.max_pool2d(h_conv1, [2, 2])\n",
    "        print_shape(h_conv1)\n",
    "        print_shape(h_pool1)\n",
    "\n",
    "        h_conv2 = slim.conv2d(h_pool1, 64, [3, 3])\n",
    "        h_pool2 = slim.max_pool2d(h_conv2, [2, 2])\n",
    "        print_shape(h_conv2)\n",
    "        print_shape(h_pool2)\n",
    "\n",
    "        h_conv3 = slim.conv2d(h_pool2, 64, [3, 3])\n",
    "        h_pool3 = slim.max_pool2d(h_conv3, [2, 2])\n",
    "        print_shape(h_conv3)\n",
    "        print_shape(h_pool3)  \n",
    "\n",
    "        ### Global (fully connect)\n",
    "        h_pool3_flat = slim.flatten(h_pool3)\n",
    "        print_shape(h_pool3_flat)\n",
    "        h_pool3_fc   = slim.fully_connected(h_pool3_flat, NN_OUTPUT_SIZE, activation_fn = None, normalizer_fn = None)\n",
    "        h_pool3_reconstract = slim.array_ops.reshape(h_pool3_fc, [-1, 4, SPLIT_SIZE, 1])\n",
    "        print_shape(h_pool3_reconstract)\n",
    "        \n",
    "        ### Local (Convolution)\n",
    "        h_pool3_conv = slim.conv2d(h_pool3, 1, [3, 3], activation_fn = None, normalizer_fn = None)\n",
    "        print_shape(h_pool3_conv)\n",
    "        \n",
    "        ### Add global and local\n",
    "        h_add = tf.add(h_pool3_reconstract, h_pool3_conv)\n",
    "        y_flat = slim.flatten(h_add)\n",
    "        print_shape(y_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Loss\n",
    "loss = tf.losses.mean_squared_error(y_, y_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Training step (Optimizer)\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Initialize session\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 train loss = 45.099233572823664, valid loss = 57.040126255580354, sqrt(valid loss) * 1.0 = 7.5524913939428195\n",
      "Epoch 2000 train loss = 3.4401085444859096, valid loss = 7.684740774972099, sqrt(valid loss) * 1.0 = 2.7721365000612974\n",
      "Epoch 3000 train loss = 1.8087670734950474, valid loss = 3.4743043490818573, sqrt(valid loss) * 1.0 = 1.86394859078298\n",
      "Epoch 4000 train loss = 0.817995994431632, valid loss = 2.2420745304652625, sqrt(valid loss) * 1.0 = 1.4973558463055008\n",
      "Epoch 5000 train loss = 1.2049815927233014, valid loss = 1.5402809517724172, sqrt(valid loss) * 1.0 = 1.24108055813167\n",
      "Epoch 6000 train loss = 0.45289166825158256, valid loss = 1.0800246034349714, sqrt(valid loss) * 1.0 = 1.0392423218070805\n",
      "Epoch 7000 train loss = 0.5516463620322092, valid loss = 1.0291009902954102, sqrt(valid loss) * 1.0 = 1.0144461495295896\n",
      "Epoch 8000 train loss = 0.33803885664258687, valid loss = 0.720585857118879, sqrt(valid loss) * 1.0 = 0.8488732868449089\n",
      "Epoch 9000 train loss = 0.20474199567522322, valid loss = 0.5194362027304513, sqrt(valid loss) * 1.0 = 0.720719226003061\n",
      "Epoch 10000 train loss = 0.25093330400330677, valid loss = 0.6731829132352557, sqrt(valid loss) * 1.0 = 0.8204772472380057\n"
     ]
    }
   ],
   "source": [
    "### Training\n",
    "EPOCH = 10000\n",
    "VALIDATION_EPOCH = 1000\n",
    "for epoch in range(1, EPOCH + 1) :\n",
    "    \n",
    "    train_loss  = 0\n",
    "    train_count = 0\n",
    "    for train_x, train_y in dataset() :\n",
    "        train_count += 1\n",
    "        _, tmp_loss = sess.run([train_step, loss], feed_dict = {x: train_x, y_: train_y, is_training: True})\n",
    "        batch_loss = tmp_loss / train_x.shape[0]\n",
    "        train_loss += batch_loss\n",
    "    if train_count > 0 :\n",
    "        train_loss /= train_count\n",
    "\n",
    "    ### Validation\n",
    "    if epoch % VALIDATION_EPOCH == 0 :\n",
    "        valid_loss  = 0\n",
    "        valid_count = 0\n",
    "        for (valid_x, valid_y) in valid_dataset() :\n",
    "            valid_count += 1\n",
    "            tmp_loss   = sess.run(loss, feed_dict = {x: valid_x, y_: valid_y, is_training: True}) # TODO (※)\n",
    "            batch_loss = tmp_loss / valid_x.shape[0]\n",
    "            valid_loss += batch_loss\n",
    "        if valid_count > 0 :\n",
    "            valid_loss /= valid_count\n",
    "        print(\"Epoch {0} train loss = {1}, valid loss = {2}, sqrt(valid loss) * {4} = {3}\".format(epoch, train_loss, valid_loss, math.sqrt(valid_loss) * NORMALIZE, NORMALIZE))\n",
    "\n",
    "# (※) 正しくは「is_training : False」とすべき。\n",
    "#      本来は valid のために batch norm 用に値を保持する必要があるが、本プログラムでは保持していないため、is_training モードが正しく動作しない。\n",
    "#      train データと valid データそれぞれの平均・分散値が近いという仮定から、ここでは保持せず、valid 時も batch norm そのまま利用している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cae_model.ckpt'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Save session\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, \"cae_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from cae_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "### Load session\n",
    "saver.restore(sess, \"cae_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
