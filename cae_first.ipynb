{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "### Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 28\n",
    "SPLIT_SIZE = 4\n",
    "\n",
    "### train_x = train_x / NORMALIZE\n",
    "NORMALIZE = 1.0\n",
    "# NORMALIZE = 255   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Dataset class\n",
    "class Dataset(object) :\n",
    "    \n",
    "    def __init__(self, path, batch_size, shuffle = True, normalize = 255.0) :\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle    = shuffle\n",
    "        self.normalize  = normalize\n",
    "        self.xs, self.ys = self._load_data(path)\n",
    "        \n",
    "        self.data_size = self.xs.shape[0]\n",
    "        self.batch_max = int(math.ceil(self.data_size / float(self.batch_size)))\n",
    "        \n",
    "    def _load_data(self, path) :\n",
    "        reader = csv.reader(open(path), lineterminator = \"\\n\")\n",
    "        data   = []\n",
    "        labels = []\n",
    "        for row in reader :\n",
    "            tmp = np.asarray([float(x) for x in row])\n",
    "            \n",
    "            ### Normalize\n",
    "            tmp = tmp / self.normalize\n",
    "            \n",
    "            data.append(tmp[:INPUT_SIZE*INPUT_SIZE])\n",
    "            labels.append(tmp[INPUT_SIZE*INPUT_SIZE:])\n",
    "        \n",
    "        data   = np.asarray(data)\n",
    "        labels = np.asarray(labels)\n",
    "        return (data, labels)\n",
    "\n",
    "    def __call__(self) :\n",
    "        indexes = None\n",
    "        if self.shuffle :\n",
    "            indexes = np.random.permutation(self.data_size)\n",
    "        else :\n",
    "            indexes = np.arange(self.data_size)\n",
    "\n",
    "        for d_idx in range(self.batch_max) :\n",
    "            start =     (d_idx + 0) * self.batch_size\n",
    "            end   = min((d_idx + 1) * self.batch_size, self.data_size)\n",
    "            \n",
    "            batch_data_size = len(indexes[start:end])\n",
    "            xs = np.zeros((batch_data_size, self.xs.shape[1]), dtype = np.float32)\n",
    "            ys = np.zeros((batch_data_size, self.ys.shape[1]), dtype = np.float32)\n",
    "            \n",
    "            for b_idx, r_idx in enumerate(indexes[start:end]) :\n",
    "                xs[b_idx] = self.xs[r_idx]\n",
    "                ys[b_idx] = self.ys[r_idx]\n",
    "\n",
    "            yield (xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load dataset\n",
    "dataset = Dataset(\"training_data_4x4.csv\", 10, shuffle = True, normalize = NORMALIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_dataset = Dataset(\"training_data_4x4.csv\", 10, shuffle = False, normalize = NORMALIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load batch data sample code\n",
    "# for batch_num, (train_x, train_y) in enumerate(dataset(), 1) :\n",
    "#     print(\"Batch {0}\".format(batch_num))\n",
    "#     print(\"Train shape : {0}\".format(train_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### for jupyter, interactive session.\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer \"Conv_3/Relu:0\" shape : (?, 28, 28, 32)\n",
      "Layer \"MaxPool2D_3/MaxPool:0\" shape : (?, 14, 14, 32)\n",
      "Layer \"Conv_4/Relu:0\" shape : (?, 14, 14, 32)\n",
      "Layer \"MaxPool2D_4/MaxPool:0\" shape : (?, 7, 7, 32)\n",
      "Layer \"fully_connected_2/Relu:0\" shape : (?, 500)\n",
      "Layer \"fully_connected_3/BiasAdd:0\" shape : (?, 16)\n"
     ]
    }
   ],
   "source": [
    "### Network settings\n",
    "NN_INPUT_SIZE  = INPUT_SIZE * INPUT_SIZE\n",
    "NN_OUTPUT_SIZE = 4 * SPLIT_SIZE\n",
    "\n",
    "x  = tf.placeholder(tf.float32, shape=[None, NN_INPUT_SIZE])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, NN_OUTPUT_SIZE])\n",
    "\n",
    "def print_shape(layer) :\n",
    "    print(\"Layer \\\"{0}\\\" shape : {1}\".format(layer.name, layer.shape))\n",
    "\n",
    "with slim.arg_scope([slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu,\n",
    "    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "    biases_initializer=tf.constant_initializer(0.1)):\n",
    "    with slim.arg_scope([slim.max_pool2d], padding='SAME'):\n",
    "        x_image = slim.array_ops.reshape(x, [-1, INPUT_SIZE, INPUT_SIZE, 1])\n",
    "        h_conv1 = slim.conv2d(x_image, 32, [5, 5])\n",
    "        h_pool1 = slim.max_pool2d(h_conv1, [2, 2])\n",
    "\n",
    "        h_conv2 = slim.conv2d(h_pool1, 32, [5, 5])\n",
    "        h_pool2 = slim.max_pool2d(h_conv2, [2, 2])\n",
    "        \n",
    "        h_pool2_flat = slim.flatten(h_pool2)\n",
    "        h_fc1 = slim.fully_connected(h_pool2_flat, 500)\n",
    "\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = slim.dropout(h_fc1, keep_prob)\n",
    "\n",
    "        y_out  = slim.fully_connected(h_fc1_drop, NN_OUTPUT_SIZE, activation_fn=None)\n",
    "        \n",
    "print_shape(h_conv1)\n",
    "print_shape(h_pool1)\n",
    "print_shape(h_conv2)\n",
    "print_shape(h_pool2)\n",
    "print_shape(h_fc1)\n",
    "print_shape(y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Loss\n",
    "loss = tf.losses.mean_squared_error(y_, y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Training step (Optimizer)\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Initialize session\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 train loss = 866.3181213378907, valid loss = 96.42523422241212, sqrt(valid loss) * 1.0 = 9.819635136929076\n",
      "Epoch 2000 train loss = 625.6139953613281, valid loss = 73.51789245605468, sqrt(valid loss) * 1.0 = 8.574257545470317\n",
      "Epoch 3000 train loss = 500.0207244873047, valid loss = 39.24505615234375, sqrt(valid loss) * 1.0 = 6.264587468648175\n",
      "Epoch 4000 train loss = 476.5160247802734, valid loss = 71.26169471740722, sqrt(valid loss) * 1.0 = 8.441664214916822\n",
      "Epoch 5000 train loss = 418.0080337524414, valid loss = 61.285160827636716, sqrt(valid loss) * 1.0 = 7.82848394183936\n",
      "Epoch 6000 train loss = 481.97813720703124, valid loss = 30.08160972595215, sqrt(valid loss) * 1.0 = 5.484670430021493\n",
      "Epoch 7000 train loss = 359.5241729736328, valid loss = 24.081302452087403, sqrt(valid loss) * 1.0 = 4.9072703667199145\n",
      "Epoch 8000 train loss = 466.69944152832034, valid loss = 26.304337596893312, sqrt(valid loss) * 1.0 = 5.128775448086348\n",
      "Epoch 9000 train loss = 436.4885925292969, valid loss = 19.69706811904907, sqrt(valid loss) * 1.0 = 4.438137911224602\n",
      "Epoch 10000 train loss = 486.26287841796875, valid loss = 34.53144454956055, sqrt(valid loss) * 1.0 = 5.876346190411228\n"
     ]
    }
   ],
   "source": [
    "### Training\n",
    "EPOCH = 10000\n",
    "VALIDATION_EPOCH = 1000\n",
    "for epoch in range(1, EPOCH + 1) :\n",
    "    \n",
    "    train_loss = 0\n",
    "    for train_x, train_y in dataset() :\n",
    "        _, tmp_loss = sess.run([train_step, loss], feed_dict = {x: train_x, y_: train_y, keep_prob: 0.5})\n",
    "        batch_loss = tmp_loss / train_x.shape[0]\n",
    "        train_loss += batch_loss\n",
    "\n",
    "    ### Validation\n",
    "    if epoch % VALIDATION_EPOCH == 0 :\n",
    "        valid_loss = 0\n",
    "        for (valid_x, valid_y) in valid_dataset() :\n",
    "            _tmp_loss  = sess.run(loss, feed_dict = {x: train_x, y_: train_y, keep_prob: 1.0})\n",
    "            batch_loss = _tmp_loss / valid_x.shape[0]\n",
    "            valid_loss += batch_loss\n",
    "        print(\"Epoch {0} train loss = {1}, valid loss = {2}, sqrt(valid loss) * {4} = {3}\".format(epoch, train_loss, valid_loss, math.sqrt(valid_loss) * NORMALIZE, NORMALIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cae_model.ckpt'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Save session\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, \"cae_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from cae_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "### Load session\n",
    "saver.restore(sess, \"cae_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
